{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/poker_dqn_03-14_22-13\n",
      "cuda\n",
      "Finished episode 0\n",
      "Finished episode 1\n",
      "Finished episode 2\n",
      "Finished episode 3\n",
      "Finished episode 4\n",
      "Finished episode 5\n",
      "Finished episode 6\n",
      "Finished episode 7\n",
      "Finished episode 8\n",
      "Finished episode 9\n",
      "Finished episode 10\n",
      "Finished episode 11\n",
      "Finished episode 12\n",
      "Finished episode 13\n",
      "Finished episode 14\n",
      "Finished episode 15\n",
      "Finished episode 16\n",
      "Finished episode 17\n",
      "Finished episode 18\n",
      "Finished episode 19\n",
      "Finished episode 20\n",
      "Finished episode 21\n",
      "Finished episode 22\n",
      "Finished episode 23\n",
      "Finished episode 24\n",
      "Finished episode 25\n",
      "Finished episode 26\n",
      "Finished episode 27\n",
      "Finished episode 28\n",
      "Finished episode 29\n",
      "Finished episode 30\n",
      "Finished episode 31\n",
      "Finished episode 32\n",
      "Finished episode 33\n",
      "Finished episode 34\n",
      "Finished episode 35\n",
      "Finished episode 36\n",
      "Finished episode 37\n",
      "Finished episode 38\n",
      "Finished episode 39\n",
      "Finished episode 40\n",
      "Finished episode 41\n",
      "Finished episode 42\n",
      "Finished episode 43\n",
      "Finished episode 44\n",
      "Finished episode 45\n",
      "Finished episode 46\n",
      "Finished episode 47\n",
      "Finished episode 48\n",
      "Finished episode 49\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import deque\n",
    "from pypokerengine.api.emulator import Emulator\n",
    "from pypokerengine.utils.card_utils import gen_cards, estimate_hole_card_win_rate\n",
    "from pypokerengine.players import BasePokerPlayer\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#GLOBAL VAR\n",
    "total_loss_p1 = 0\n",
    "total_loss_p2 = 0\n",
    "total_rewards_p1 = 0\n",
    "total_rewards_p2 = 0\n",
    "starting_stack = 1000\n",
    "num_wins_p1 = 0\n",
    "num_wins_p2 = 0\n",
    "total_profits_p1 = 0\n",
    "total_profits_p2 = 0\n",
    "amount_won_p1 = 0\n",
    "amount_won_p2 = 0\n",
    "\n",
    "\n",
    "\n",
    "# Define Experience Replay class\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "       \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Define Poker DQN\n",
    "class PokerDQN(torch.nn.Module):\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(PokerDQN, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(n_states, 128)\n",
    "        self.layer2 = torch.nn.Linear(128, 128)\n",
    "        self.layer3 = torch.nn.Linear(128, n_actions)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.layer1(x))\n",
    "        x = torch.nn.functional.relu(self.layer2(x))\n",
    "        return self.layer3(x)  # Outputs Q-values for Fold, Call, Raise\n",
    "\n",
    "# Define a custom player class\n",
    "class DQNPlayer(BasePokerPlayer):\n",
    "    def __init__(self, uuid, policy_net, target_net, optimizer, memory):\n",
    "        self.uuid = uuid\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optimizer\n",
    "        self.memory = memory\n",
    "        self.steps_done = 0\n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, game_state, round_state):\n",
    "        \"\"\" Choose an action based on the DQN's predicted Q-values with epsilon-greedy exploration. \"\"\"\n",
    "        #Retrieve hole cards through player object\n",
    "\n",
    "        # Encode the state based on the filtered hole cards\n",
    "        state = self.encode_state(hole_card, round_state, starting_stack)\n",
    "        \n",
    "        action_index = self.select_action(state, valid_actions)  #0 = fold, 1 = call, 2 = raise\n",
    "        # Retrieve corresponding action name\n",
    "        action = valid_actions[action_index][\"action\"]\n",
    "\n",
    "        # Handle amount selection for \"raise\" action\n",
    "        amount = valid_actions[action_index].get(\"amount\", 0)\n",
    "        if isinstance(amount, dict): \n",
    "            amount = amount.get(\"min\", 0)                                                                             \n",
    "        \n",
    "\n",
    "\n",
    "        #always default to action = call 0 (check)\n",
    "        if action_index == 0 and valid_actions[1][\"amount\"] == 0:\n",
    "            return \"call\", 0\n",
    "        \n",
    "        return action, amount  \n",
    "\n",
    "    def get_hole_cards(self, game_state):\n",
    "        \"\"\" Retrieve hole cards for this player using the player object. \"\"\"\n",
    "        for player in game_state[\"table\"].seats.players:\n",
    "            if player.uuid == self.uuid:\n",
    "                card1 = str(player.hole_card[0])\n",
    "                card2 = str(player.hole_card[1])\n",
    "                return card1, card2  # Get hole cards through player object\n",
    "\n",
    "    def select_action(self, state, valid_actions):\n",
    "        \"\"\" Epsilon-greedy action selection: sometimes random, sometimes best Q-value \"\"\"\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) *  math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        state = state.to(torch.float32)\n",
    "        \n",
    "\n",
    "        writer.add_scalar(f\"Exploration/{self.uuid}_Epsilon\", eps_threshold, self.steps_done)\n",
    "\n",
    "        if random.random() > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state)\n",
    "                writer.add_scalar(f\"Call Q-Values/{self.uuid}\", q_values[0][0], self.steps_done)\n",
    "                writer.add_scalar(f\"Fold Q-Values/{self.uuid}\", q_values[0][1], self.steps_done)\n",
    "                writer.add_scalar(f\"Raise Q-Values/{self.uuid}\", q_values[0][2], self.steps_done)\n",
    "                return q_values.argmax(1).item()\n",
    "        else:\n",
    "            return random.choice(range(len(valid_actions)))\n",
    "\n",
    "    def encode_state(self, hole_card, round_state, starting_stack):\n",
    "        \"\"\" Convert poker game state into a fixed-length vector suitable for the DQN. \"\"\"\n",
    "        hand_strength = estimate_hole_card_win_rate(\n",
    "            nb_simulation=50, \n",
    "            nb_player=2, \n",
    "            hole_card=gen_cards([hole_card[0],hole_card[1]]),  \n",
    "            community_card=gen_cards(round_state[\"community_card\"])\n",
    "        )\n",
    "\n",
    "        pot_size = round_state[\"pot\"][\"main\"][\"amount\"] / starting_stack\n",
    "        if round_state[\"seats\"][0][\"uuid\"] == self.uuid:\n",
    "            stack_size = round_state[\"seats\"][0][\"stack\"] / starting_stack\n",
    "        else: stack_size = round_state[\"seats\"][1][\"stack\"] / starting_stack\n",
    "\n",
    "\n",
    "        bb_position = int(round_state[\"big_blind_pos\"] == 0)\n",
    "        opponent_uuid = \"p2_uuid\" if self.uuid == \"p1_uuid\" else \"p1_uuid\"\n",
    "        opponent_action = 0  # Default action index\n",
    "        opponent_bet_size = 0\n",
    "        \n",
    "        if opponent_uuid in round_state[\"action_histories\"]:\n",
    "            street = round_state[\"street\"]\n",
    "            history = round_state[\"action_histories\"][street]\n",
    "            if history:\n",
    "                opponent_action = history[-1][\"action\"]\n",
    "                opponent_bet_size = history[-1][\"amount\"] / starting_stack\n",
    "        \n",
    "        state_vector = np.array([hand_strength, pot_size, stack_size, bb_position, opponent_action, opponent_bet_size], dtype=np.float32)\n",
    "        return torch.tensor(state_vector, device=device).unsqueeze(0)\n",
    "\n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        pass\n",
    "\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "def soft_update(target_net, policy_net, tau):\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    \"\"\"Processes a batch of transitions into tensors.\"\"\"\n",
    "    state_batch = torch.cat([b[0] for b in batch]).to(device)\n",
    "    action_batch = torch.tensor([b[1] for b in batch], device=device).unsqueeze(1)\n",
    "    reward_batch = torch.tensor([b[2] for b in batch], device=device).unsqueeze(1)\n",
    "\n",
    "    non_final_mask = torch.tensor([b[3] is not None for b in batch], device=device, dtype=torch.bool)\n",
    "    if non_final_mask.any():\n",
    "        non_final_next_states = torch.cat([b[3] for b in batch if b[3] is not None]).to(device)\n",
    "    else:\n",
    "        non_final_next_states = None\n",
    "\n",
    "    return state_batch, action_batch, reward_batch, non_final_next_states, non_final_mask\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Optimizes both players separately using their own replay memory.\"\"\"\n",
    "    global total_loss_p1, total_loss_p2\n",
    "    #Optimize Player 1\n",
    "    if len(memory1) >= BATCH_SIZE and len(memory2) >= BATCH_SIZE:\n",
    "        batch_p1 = memory1.sample(BATCH_SIZE)\n",
    "        state_p1, action_p1, reward_p1, next_state_p1, mask_p1 = process_batch(batch_p1)\n",
    "\n",
    "        q_values_p1 = p1_policy_net(state_p1).gather(1, action_p1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q_values_p1 = reward_p1.clone()\n",
    "            if mask_p1.any():\n",
    "                next_actions_p1 = p1_policy_net(next_state_p1).argmax(1, keepdim=True)  #Policy net selects best action\n",
    "                max_next_q_values_p1 = p1_target_net(next_state_p1).gather(1, next_actions_p1)  #Target net evaluates action\n",
    "                target_q_values_p1[mask_p1] += GAMMA * max_next_q_values_p1\n",
    "                \n",
    "        loss_p1 = torch.nn.functional.smooth_l1_loss(q_values_p1, target_q_values_p1)\n",
    "\n",
    "        optimizer_p1.zero_grad()\n",
    "        loss_p1.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(p1_policy_net.parameters(), 1.0)\n",
    "\n",
    "        optimizer_p1.step()\n",
    "\n",
    "        soft_update(p1_target_net, p1_policy_net, TAU)\n",
    "\n",
    "        writer.add_scalar(\"Loss/P1_Loss\", loss_p1.item(), dqn_player1.steps_done)\n",
    "        writer.add_scalar(\"Q-Values/P1_Max\", q_values_p1.max().item(), dqn_player1.steps_done)\n",
    "        writer.add_scalar(\"Q-Values/P1_Mean\", q_values_p1.mean().item(), dqn_player1.steps_done)\n",
    "        total_loss_p1 += loss_p1.item()\n",
    "\n",
    "    # Optimize Player 2\n",
    "        batch_p2 = memory2.sample(BATCH_SIZE)\n",
    "        state_p2, action_p2, reward_p2, next_state_p2, mask_p2 = process_batch(batch_p2)\n",
    "\n",
    "        q_values_p2 = p2_policy_net(state_p2).gather(1, action_p2)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q_values_p2 = reward_p2.clone()\n",
    "            if mask_p2.any():\n",
    "                next_actions_p2 = p2_policy_net(next_state_p2).argmax(1, keepdim=True)  #Policy net selects best action\n",
    "                max_next_q_values_p2 = p2_target_net(next_state_p2).gather(1, next_actions_p2)  #Target net evaluates action\n",
    "                target_q_values_p2[mask_p2] += GAMMA * max_next_q_values_p2\n",
    "\n",
    "        loss_p2 = torch.nn.functional.smooth_l1_loss(q_values_p2, target_q_values_p2)\n",
    "\n",
    "        optimizer_p2.zero_grad()\n",
    "        loss_p2.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(p2_policy_net.parameters(), 1.0)\n",
    "\n",
    "        optimizer_p2.step()\n",
    "\n",
    "        soft_update(p2_target_net, p2_policy_net, TAU)\n",
    "\n",
    "        writer.add_scalar(\"Loss/P2_Loss\", loss_p2.item(), dqn_player2.steps_done)\n",
    "        writer.add_scalar(\"Q-Values/P2_Max\", q_values_p2.max().item(), dqn_player2.steps_done)\n",
    "        writer.add_scalar(\"Q-Values/P2_Mean\", q_values_p2.mean().item(), dqn_player2.steps_done)\n",
    "        total_loss_p2 += loss_p2.item()\n",
    "\n",
    "\n",
    "def calculate_reward(player_uuid, events, prev_stack, action, prev_hand_strength, new_hand_strength, winners,):\n",
    "    round_state = events[-1][\"round_state\"]\n",
    "    \n",
    "    # Get player's final stack after the action\n",
    "    current_stack = next(seat[\"stack\"] for seat in round_state[\"seats\"] if seat[\"uuid\"] == player_uuid)\n",
    "\n",
    "    # stack-based Reward (Normalized by Big Blind)\n",
    "    stack_reward = (current_stack - prev_stack) / 10  \n",
    "\n",
    "    # Free Card\n",
    "    check_reward = 0.02 if action == \"call\" and round_state[\"pot\"][\"main\"][\"amount\"] == prev_stack else 0\n",
    "\n",
    "    #Hand Strength Improvement Reward\n",
    "    hand_strength_reward = (new_hand_strength - prev_hand_strength) * 0.5  \n",
    "\n",
    "    # Win/Loss Reward\n",
    "    win_reward = 0\n",
    "    if winners:\n",
    "        win_reward = 1.0 if player_uuid == events[-1][\"winners\"][0][\"uuid\"] else -1.0  \n",
    "\n",
    "    # Fold Penalty \n",
    "    fold_penalty = -0.05 if action == \"fold\" else 0  \n",
    "\n",
    "    # Compute Final Reward\n",
    "    total_reward = stack_reward + check_reward + hand_strength_reward + win_reward + fold_penalty\n",
    "    \n",
    "    # Normalize & Clip Reward Between [-1, 1]\n",
    "    total_reward = np.clip(total_reward, -1, 1)\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "from pypokerengine.utils.game_state_utils import restore_game_state, attach_hole_card_from_deck\n",
    "\n",
    "\n",
    "def train_poker_bot():\n",
    "    global total_profits_p1\n",
    "    global total_profits_p2\n",
    "    global amount_won_p1\n",
    "    global amount_won_p2\n",
    "    p1_prev_stack = 0\n",
    "    p2_prev_stack = 0\n",
    "    p1_action = \"blind\"\n",
    "    p2_action = \"blind\"\n",
    "    p1_prev_hand_strength = 0\n",
    "    p2_prev_hand_strength = 0\n",
    "    game_state, events = emulator.start_new_round(initial_game_state)\n",
    "\n",
    "    global total_rewards_p1\n",
    "    global total_rewards_p2\n",
    "\n",
    "    roundcounter = 0\n",
    "    x_axis = 0\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        while True:\n",
    "            '''\n",
    "            if episode % 50 == 0:\n",
    "                print(events[-1][\"round_state\"][\"street\"])\n",
    "                print(events[-1][\"round_state\"][\"pot\"][\"main\"][\"amount\"])\n",
    "                print(events[-1][\"round_state\"][\"community_card\"])'''\n",
    "            try:\n",
    "                events[-1][\"uuid\"]  # Attempt to access\n",
    "            except (IndexError, KeyError):\n",
    "                optimize_model()\n",
    "                game_state, events = emulator.start_new_round(initial_game_state)\n",
    "                break\n",
    "            if events[-1][\"uuid\"] == \"p1_uuid\":\n",
    "                p2_hole_cards = dqn_player2.get_hole_cards(game_state)\n",
    "                p2_state = dqn_player1.encode_state(p2_hole_cards, events[-1][\"round_state\"], starting_stack)\n",
    "                p1_action, amount_p1 = dqn_player1.declare_action(\n",
    "                    events[-1][\"valid_actions\"], \n",
    "                    dqn_player1.get_hole_cards(game_state),\n",
    "                    game_state,  # ✅ Get hole cards from player object\n",
    "                    events[-1][\"round_state\"]) \n",
    "                game_state, events = emulator.apply_action(game_state, p1_action, amount_p1)\n",
    "                if events[-1][\"type\"] == \"event_game_finish\":\n",
    "                    optimize_model()\n",
    "                    game_state, events = emulator.start_new_round(initial_game_state)\n",
    "\n",
    "                    \n",
    "                    break\n",
    "\n",
    "                p2_next_state = dqn_player2.encode_state(p2_hole_cards, events[-1][\"round_state\"], starting_stack)\n",
    "                p2_new_hand_strength = estimate_hole_card_win_rate(nb_simulation=50, nb_player=2, hole_card=gen_cards([*p2_hole_cards]), community_card=gen_cards(events[-1][\"round_state\"][\"community_card\"]))\n",
    "                \n",
    "                reward2 = calculate_reward(\"p2_uuid\", events, p1_prev_stack, p1_action, p1_prev_hand_strength, p2_new_hand_strength, events[-1][\"type\"] == \"event_round_finish\")\n",
    "                writer.add_scalar(\"P2 Rewards\", reward2, x_axis)\n",
    "                total_rewards_p2 += reward2\n",
    "                done = events[-1][\"type\"] == \"event_round_finish\" or events[-1][\"type\"] == \"event_game_finish\"\n",
    "                memory2.push(p2_state, actions.index(p1_action.upper()), reward2, p2_next_state, done)\n",
    "\n",
    "                writer.add_scalar(\"Actions/Player1_Action\", actions.index(p1_action.upper()), episode)\n",
    "\n",
    "            else:\n",
    "                p1_hole_cards = dqn_player1.get_hole_cards(game_state)\n",
    "                p1_state = dqn_player1.encode_state(dqn_player1.get_hole_cards(game_state), events[-1][\"round_state\"], starting_stack)\n",
    "                p2_action, amount_p2 = dqn_player2.declare_action(\n",
    "                                events[-1][\"valid_actions\"], \n",
    "                                dqn_player2.get_hole_cards(game_state),\n",
    "                                game_state,   # ✅ Get hole cards from player object\n",
    "                                events[-1][\"round_state\"])\n",
    "                game_state, events = emulator.apply_action(game_state, p2_action, amount_p2)\n",
    "                if events[-1][\"type\"] == \"event_game_finish\":\n",
    "                    optimize_model()\n",
    "\n",
    "                    game_state, events = emulator.start_new_round(initial_game_state)\n",
    "\n",
    "                    \n",
    "                    break\n",
    "\n",
    "\n",
    "                p1_next_state = dqn_player1.encode_state(p1_hole_cards, events[-1][\"round_state\"], starting_stack)\n",
    "                p1_new_hand_strength = estimate_hole_card_win_rate(nb_simulation=50, nb_player=2, hole_card=gen_cards([*p1_hole_cards]), community_card=gen_cards(events[-1][\"round_state\"][\"community_card\"]))\n",
    "                reward1 = calculate_reward(\"p1_uuid\", events, p2_prev_stack, p2_action, p2_prev_hand_strength, p1_new_hand_strength, events[-1][\"type\"] == \"event_round_finish\")\n",
    "                total_rewards_p1 += reward1\n",
    "                writer.add_scalar(\"P1 Rewards\", reward1, x_axis)\n",
    "                done = events[-1][\"type\"] == \"event_round_finish\" or events[-1][\"type\"] == \"event_game_finish\"\n",
    "                memory1.push(p1_state, actions.index(p2_action.upper()), reward1, p1_next_state, done)\n",
    "                \n",
    "                \n",
    "                writer.add_scalar(\"Actions/Player2_Action\", actions.index(p2_action.upper()), episode)\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "            global num_wins_p1\n",
    "            global num_wins_p2\n",
    "            if events[-1][\"type\"] == \"event_round_finish\":              \n",
    "                if events[-1]['winners'][0] == \"p1_uuid\":\n",
    "                    num_wins_p1 += 1\n",
    "                    amount_won_p2 += events[-1]['winners'][0]['stack']\n",
    "                else:\n",
    "                    num_wins_p2 += 1\n",
    "                    amount_won_p2 += events[-1]['winners'][0]['stack']\n",
    "\n",
    "                p1_current_stack = events[-1][\"round_state\"][\"seats\"][0][\"stack\"]\n",
    "                total_profits_p1 += (p1_current_stack - starting_stack)\n",
    "                writer.add_scalar(\"Game/P1 Profits\", total_profits_p1, x_axis)  \n",
    "                \n",
    "                p2_current_stack = events[-1][\"round_state\"][\"seats\"][1][\"stack\"]\n",
    "                total_profits_p2 += (p2_current_stack - starting_stack)\n",
    "                writer.add_scalar(\"Game/P2 Profits\", total_profits_p2, x_axis) \n",
    "                game_state, events = emulator.start_new_round(game_state)\n",
    "\n",
    "                roundcounter += 1\n",
    "            \n",
    "            x_axis += 1\n",
    "     \n",
    "            \n",
    "\n",
    "        \n",
    "            optimize_model()\n",
    "        roundcounter = 0\n",
    "        print(f\"Finished episode {episode}\")\n",
    "    \n",
    "           \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "log_dir = \"runs/poker_dqn_\" + datetime.datetime.now().strftime(\"%m-%d_%H-%M\")\n",
    "print(log_dir)\n",
    "writer = SummaryWriter(log_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "emulator = Emulator()\n",
    "\n",
    "###############################\n",
    "#ADJUST NUMBER OF ROUNDS HERE\n",
    "###############################\n",
    "emulator.set_game_rule(player_num=2, max_round=100, small_blind_amount=5, ante_amount=0)\n",
    "\n",
    "\n",
    "players_info = {\n",
    "    \"p1_uuid\": {\"name\": \"Player 1\", \"stack\": starting_stack},\n",
    "    \"p2_uuid\": {\"name\": \"Player 2\", \"stack\": starting_stack}\n",
    "}\n",
    "\n",
    "initial_game_state = emulator.generate_initial_game_state(players_info)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 50\n",
    "BATCH_SIZE = 128                         #min num of rounds before replay mem\n",
    "GAMMA = 0.99                            #higher prio long term rewards\n",
    "EPS_START = 1.0                         #100% rand at start\n",
    "EPS_END = 0.2                           #10% rand at end                            \n",
    "EPS_DECAY = 2000                         #decay speed             \n",
    "TAU = 0.005                             #how oftern target is updated by policy\n",
    "LR = 1e-5                               #learing rate\n",
    "MEMORY_CAPACITY = 10000\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"eps_start\": EPS_START,\n",
    "    \"eps_end\": EPS_END,\n",
    "    \"eps_decay\": EPS_DECAY,\n",
    "    \"tau\": TAU,\n",
    "    \"learning_rate\": LR,\n",
    "    \"memory_capacity\": MEMORY_CAPACITY\n",
    "}\n",
    "\n",
    "\n",
    "###############################\n",
    "#ADJUST NUMBER OF ROUNDS ABOVE\n",
    "###############################\n",
    "\n",
    "actions = [\"FOLD\", \"CALL\", \"RAISE\"]\n",
    "n_actions = len(actions)\n",
    "n_states = 6  # Hand strength, pot size, stack size, BB position, opp last action, opp bet size\n",
    "\n",
    "p1_policy_net = PokerDQN(n_states, n_actions).to(device)\n",
    "p1_target_net = PokerDQN(n_states, n_actions).to(device)\n",
    "p1_target_net.load_state_dict(p1_policy_net.state_dict())\n",
    "p1_target_net.eval()\n",
    "\n",
    "p2_policy_net = PokerDQN(n_states, n_actions).to(device)\n",
    "p2_target_net = PokerDQN(n_states, n_actions).to(device)\n",
    "p2_target_net.load_state_dict(p2_policy_net.state_dict())\n",
    "p2_target_net.eval()\n",
    "\n",
    "optimizer_p1 = torch.optim.AdamW(p1_policy_net.parameters(), lr=LR)\n",
    "optimizer_p2 = torch.optim.AdamW(p2_policy_net.parameters(), lr=LR)\n",
    "\n",
    "memory1 = ReplayMemory(MEMORY_CAPACITY)\n",
    "memory2 = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "dqn_player1 = DQNPlayer(\"p1_uuid\", p1_policy_net, p1_target_net, optimizer_p1, memory1)\n",
    "dqn_player2 = DQNPlayer(\"p2_uuid\", p2_policy_net, p2_target_net, optimizer_p2, memory2)\n",
    "\n",
    "emulator.register_player(\"p1_uuid\", dqn_player1)\n",
    "emulator.register_player(\"p2_uuid\", dqn_player2)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "train_poker_bot()\n",
    "final_metrics = {\n",
    "    \"final_loss_p1\": total_loss_p1 / num_episodes,\n",
    "    \"final_loss_p2\": total_loss_p2 / num_episodes,\n",
    "    \"avg_reward_p1\": total_rewards_p1 / num_episodes,\n",
    "    \"avg_reward_p2\": total_rewards_p2 / num_episodes,\n",
    "\n",
    "    }\n",
    "\n",
    "writer.add_scalar(\"P1 Average Win Size\", amount_won_p1 / (num_wins_p1+1), 0)\n",
    "writer.add_scalar(\"P2 Average Win Size\", amount_won_p2 / (num_wins_p2+1), 0)\n",
    "\n",
    "writer.add_hparams(hparams, final_metrics)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define directory to save models\n",
    "model_dir = \"saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)  # ✅ Create directory if it doesn't exist\n",
    "\n",
    "# Save Player 1's model\n",
    "torch.save(p1_policy_net.state_dict(), os.path.join(model_dir, \"poker_p1_dqn.pth decay 3000\"))\n",
    "\n",
    "# Save Player 2's model\n",
    "torch.save(p2_policy_net.state_dict(), os.path.join(model_dir, \"poker_p2_dqn.pth decay 3000\"))\n",
    "\n",
    "print(\"✅ Model weights saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded Player 1's model weights.\n",
      "✅ Loaded Player 2's model weights.\n"
     ]
    }
   ],
   "source": [
    "# Load Player 1's Model (if available)\n",
    "p1_model_path = os.path.join(model_dir, \"poker_p1_dqn.pth loser\")\n",
    "if os.path.exists(p1_model_path):\n",
    "    p1_policy_net.load_state_dict(torch.load(p1_model_path))\n",
    "    print(\"✅ Loaded Player 1's model weights.\")\n",
    "#p1_policy_net.load_state_dict(torch.load(os.path.join(model_dir, \"poker_p1_dqn.pth\"))\n",
    "\n",
    "\n",
    "# Load Player 2's Model (if available)\n",
    "p2_model_path = os.path.join(model_dir, \"poker_p1_dqn.pth loser\")\n",
    "if os.path.exists(p2_model_path):\n",
    "    p2_policy_net.load_state_dict(torch.load(p2_model_path))\n",
    "    print(\"✅ Loaded Player 2's model weights.\")\n",
    "\n",
    "#p2_policy_net.load_state_dict(torch.load(os.path.join(model_dir, \"poker_p2_dqn.pth\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
